{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/pietr/Desktop/Python/Work_Project/simplewiki-2020-11-01.jsonl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"simplewiki-2020-11-01.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(f\"Error: The file '{path}' does not exist.\")\n",
    "\n",
    "with gzip.open(\n",
    "    path, \"rt\", encoding=\"utf-8\"\n",
    ") as f:  # Use gzip.open() for compressed files\n",
    "    data = [json.loads(line) for line in f]  # Read each line as JSON\n",
    "\n",
    "if len(data) > 10:\n",
    "    print(data[10])  # Print the 11th JSON object\n",
    "else:\n",
    "    print(\"Warning: Not enough lines in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'paragraphs' column exists and drop NaN values\n",
    "# if \"paragraphs\" not in df.columns:\n",
    "# raise KeyError(\"Error: 'paragraphs' column not found in DataFrame!\")\n",
    "\n",
    "if \"paragraphs\" in df and not df[\"paragraphs\"].isna().all():\n",
    "    df[\"paragraphs\"] = df[\"paragraphs\"].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"paragraphs\"])  # Remove rows with NaN values\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Load a model (modify if needed)\n",
    "\n",
    "# Encode paragraphs\n",
    "corpus_embedding = model.encode(df[\"paragraphs\"].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the queries to test the model\n",
    "\n",
    "queries = [\"Kevin Dunn\"]\n",
    "\n",
    "corpus = df[\"paragraphs\"].tolist()\n",
    "\n",
    "top_k = min(5, len(corpus))  # It determines the smaller of two values:\n",
    "# 5 and len(corpus), where len(corpus) represents the length of the list, string, or iterable named corpus.\n",
    "\n",
    "# initiate a loop to show the 5 closest results to each query, taken from the corpus\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    manual_similarities = util.pytorch_cos_sim(query_embedding, corpus_embedding)[0]\n",
    "    # model.similarity(query_embedding, corpus_embedding) likely returns a single array of similarity scores for the query.\n",
    "    scores, indices = torch.topk(manual_similarities, k=top_k)\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "    for score, idx in zip(scores, indices):\n",
    "        print(df.iloc[idx][\"paragraphs\"], f\"(Score: {score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
